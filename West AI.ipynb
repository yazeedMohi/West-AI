{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0124 15:40:05.476831  6304 __init__.py:308] Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from operator import add\n",
    "import keras\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.core import Activation, Dense, Flatten\n",
    "from keras.layers.merge import Add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\"\"\"\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Activation, Dense, Flatten\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\"\"\"\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self,all_cards=None,turn = 1, score1 = 0,score2 = 0):\n",
    "        self.finish = False\n",
    "        r=0\n",
    "        i=0\n",
    "        if(all_cards == None):\n",
    "            self.all_cards = [[],[],[],[]]\n",
    "            nums = list(range(0,52))\n",
    "            #print(nums)\n",
    "            random.shuffle(nums)\n",
    "            for n in nums:\n",
    "                self.all_cards[math.floor(i/13)].append(n)\n",
    "                i+=1\n",
    "        #print(self.all_cards)\n",
    "        self.deck = []\n",
    "        self.round = 1\n",
    "        self.pack = 1\n",
    "        self.ATO = random.randint(0,3)\n",
    "        self.finish = False\n",
    "        self.turn = 1\n",
    "        self.ATO_count = random.randint(7,10)\n",
    "        self.score1 = 0\n",
    "        self.score2 = 0\n",
    "        self.played_cards = []\n",
    "    def play_card(self,which):\n",
    "        #print((self.turn-1))\n",
    "        #print(which)\n",
    "        #print(self.all_cards[self.turn-1])\n",
    "        if(which>=len(self.all_cards[self.turn-1])):which = 0\n",
    "        v = self.all_cards[self.turn-1][which]\n",
    "        if(not self.is_playable(v)):\n",
    "            return -1\n",
    "        self.deck.append(self.all_cards[self.turn-1][which])\n",
    "        self.played_cards.append(self.all_cards[self.turn-1][which])\n",
    "        del[self.all_cards[self.turn-1][which]]\n",
    "        self.pack+=1\n",
    "        self.turn+=1\n",
    "        if(self.turn>4): self.turn=1\n",
    "        if(self.pack>4): \n",
    "            self.end_round()\n",
    "            self.pack = 1\n",
    "            self.deck.clear()\n",
    "            self.round += 1\n",
    "            if(len(self.all_cards[0])==0):\n",
    "                self.finish = True\n",
    "        return 1\n",
    "    def is_playable(self,which):\n",
    "        if len(self.deck) == 0:\n",
    "            return True\n",
    "        tabclr = math.floor(self.deck[0]/13)\n",
    "        clr = math.floor(which/13)\n",
    "        if(tabclr == clr):\n",
    "            return True\n",
    "        if(which in self.all_cards[self.turn-1]):\n",
    "            return True\n",
    "        return False\n",
    "    def get_val(self,card):\n",
    "        clr = math.floor(card/13)\n",
    "        val = card%13\n",
    "        if(val==0): val=13\n",
    "        if(clr==self.ATO): val*= 10\n",
    "        return val\n",
    "    def end_round(self):\n",
    "        besti = -1\n",
    "        i = 0\n",
    "        maxi = -9990\n",
    "        for crd in self.deck:\n",
    "            if(self.get_val(crd)>maxi):\n",
    "                maxi = self.get_val(crd)\n",
    "                besti = i\n",
    "            i += 1\n",
    "        self.turn = besti +1\n",
    "        if(besti==0 or besti == 2):\n",
    "            self.score1 += 1\n",
    "        else:\n",
    "            self.score2 += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self,PType=\"human\",agent = None,depth=3):\n",
    "        self.type = PType\n",
    "        self.depth = depth\n",
    "        if(PType==\"DQN\"):\n",
    "            if(agent != None): self.agent = agent\n",
    "            else: self.agent = DQNAgent()\n",
    "    def get_move(self,game,leng):\n",
    "        if(self.type == \"human\"):\n",
    "            return int(input(\"Enter move:\"))\n",
    "        elif(self.type == \"random\"):\n",
    "            #print(\"LPO\")\n",
    "            #print(leng)\n",
    "            return np.random.randint(0,leng-1)\n",
    "        elif(self.type==\"DQN\"):\n",
    "            return np.argmax(self.agent.model.predict(np.array(game.board).reshape((1,10)))[0])\n",
    "        elif(self.type==\"minmax\"):\n",
    "            return min_max(game.board,self.depth,game.turn,game.score1,game.score2)\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(player1,player2,player3,player4,numGames):\n",
    "    win1 = 0\n",
    "    win2 = 0\n",
    "    tie = 0\n",
    "    for gm in range(numGames):\n",
    "        print(gm+1)\n",
    "        g=Game()\n",
    "        while(not g.finish):\n",
    "            r=-1\n",
    "            while(r==-1):\n",
    "                if(g.turn==1):m =   player1.get_move(g,len(g.all_cards[0]))\n",
    "                elif(g.turn==2):m = player2.get_move(g,len(g.all_cards[1]))\n",
    "                elif(g.turn==3):m = player3.get_move(g,len(g.all_cards[2]))\n",
    "                elif(g.turn==4):m = player4.get_move(g,len(g.all_cards[3]))\n",
    "                r = g.play_card(m)\n",
    "                #print(r)\n",
    "            #if(player1.type==\"human\" or player2.type==\"human\"): print(m);g.print_game()\n",
    "        if(g.score1>g.score2): win1 += 1\n",
    "        elif(g.score1<g.score2): win2 += 1\n",
    "        else: tie += 1\n",
    "        print(\"score 1: \",g.score1)\n",
    "        print(\"score 2: \",g.score2)\n",
    "        del(g)\n",
    "    print(\"Win 1 =\",win1,\"Win 2 =\",win2,\"Tie =\",tie)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "\n",
    "    def __init__(self,index):\n",
    "        self.reward = 0\n",
    "        self.gamma = 0.9\n",
    "        self.dataframe = pd.DataFrame()\n",
    "        self.short_memory = np.array([])\n",
    "        self.agent_target = 1\n",
    "        self.agent_predict = 0\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self.network()\n",
    "        #self.model = self.network(\"weights.hdf5\")\n",
    "        self.epsilon = 0\n",
    "        self.actual = []\n",
    "        self.memory = []\n",
    "        self.index = index\n",
    "    def get_state(self, game):\n",
    "        my_cards = np.zeros([4,13])\n",
    "        i = 0\n",
    "        for card in game.all_cards[game.turn-1]:\n",
    "            for j in range(4):\n",
    "                my_cards[j][i] = card\n",
    "            i+=1\n",
    "            \n",
    "        played_cards = np.zeros([4,13])\n",
    "        i=0\n",
    "        for card in game.played_cards:\n",
    "            played_cards[math.floor(i/13)][i%13] = card\n",
    "            i+=1\n",
    "        deck = np.zeros([4,13])\n",
    "        \n",
    "        for k in range(len(game.deck)):\n",
    "            for i in range(13):\n",
    "                deck[k][i] = game.deck[k]\n",
    "        ato_type = np.dot(np.ones(52).reshape([4,13]),game.ATO)\n",
    "        ato_count = np.dot(np.ones(52).reshape([4,13]),game.ATO_count)\n",
    "        state = np.zeros([5,4,13])\n",
    "        state[0]=my_cards\n",
    "        state[1]=played_cards\n",
    "        state[2]=deck\n",
    "        state[3]=ato_type\n",
    "        state[4]=ato_count\n",
    "        #print(np.asarray(state))\n",
    "        return np.asarray(state)\n",
    "    def set_reward(self, old_delta, new_delta):\n",
    "        self.reward = 0\n",
    "        self.reward = (new_delta - old_delta) * 10\n",
    "        if(self.index ==1 or self.index==3): self.reward *= -1 \n",
    "        return self.reward\n",
    "\n",
    "    def network(self, weights=None):\n",
    "        model = get_model(cnn_filter_num=32,cnn_first_filter_size=5,l2_reg=1e-4,value_fc_size=13,cnn_filter_size=3,res_layer_num=5,n_labels=13)\n",
    "        opt = Adam()\n",
    "        losses = ['categorical_crossentropy']\n",
    "        model.compile(optimizer=opt, loss=losses, loss_weights=[1.25])\n",
    "\n",
    "        if weights:\n",
    "            model.load_weights(weights)\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state,done):\n",
    "        self.memory.append((state, action, reward, next_state,done))\n",
    "\n",
    "    def replay_new(self, memory):\n",
    "        if len(memory) > 100:\n",
    "            minibatch = random.sample(memory, 100)\n",
    "        else:\n",
    "            minibatch = memory\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state.reshape(1,5,4,13)))\n",
    "            target_f = self.model.predict(state.reshape(1,5,4,13))\n",
    "            target_f[0][np.argmax(action)] = target\n",
    "            self.model.fit(state.reshape(1,5,4,13), target_f, epochs=1, verbose=0)\n",
    "        \n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + self.gamma * np.amax(self.model.predict(next_state.reshape(1,5,4,13)))\n",
    "        p = self.model.predict(state.reshape(1,5,4,13))\n",
    "        #print(p.shape)\n",
    "        target_f = self.model.predict(state.reshape(1,5,4,13))\n",
    "        #print(target_f.shape,np.argmax(action))\n",
    "        target_f[0][np.argmax(action)] = target\n",
    "        self.model.fit(state.reshape(1,5,4,13), target_f, epochs=1, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cnn_filter_num,cnn_first_filter_size,l2_reg,value_fc_size,cnn_filter_size,res_layer_num,n_labels):\n",
    "    in_x = x = Input((5,4,13))\n",
    "    \n",
    "    x = Conv2D(filters=cnn_filter_num, kernel_size=cnn_first_filter_size, padding= \"same\",\n",
    "              data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(l2_reg),\n",
    "              name=\"input_conv\"+str(cnn_first_filter_size)+\"-\"+str(cnn_filter_num))(x)\n",
    "    x = BatchNormalization(axis=-1, name=\"input_batchnorm\")(x)\n",
    "    x = Activation(\"relu\", name=\"input_relu\")(x)\n",
    "    \n",
    "    for i in range(res_layer_num):\n",
    "        x = build_residual_block(x, i+1,cnn_filter_num,l2_reg,cnn_filter_size)\n",
    "    \n",
    "    res_out = x\n",
    "    \n",
    "    x = Conv2D(filters=2, kernel_size=1, data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(l2_reg),\n",
    "              name=\"policy_conv-1-2\")(res_out)\n",
    "    x = BatchNormalization(axis=-1, name=\"policy_batchnorm\")(x)\n",
    "    x = Activation(\"relu\", name=\"policy_relu\")(x)\n",
    "    x = Flatten(name=\"policy_flatten\")(x)\n",
    "    policy_out = Dense(n_labels,kernel_regularizer=l2(l2_reg), activation=\"softmax\", name=\"policy_out\")(x)\n",
    "    \"\"\"\n",
    "    x = Conv2D(filters=4, kernel_size=1, data_format=\"channels_first\", use_bias=False, kernel_regularizer=l2(l2_reg),\n",
    "              name=\"value_conv-1-4\")(res_out)\n",
    "    x = BatchNormalization(axis=-1, name=\"value_batchnorm\")(x)\n",
    "    x = Activation(\"relu\", name=\"value_relu\")(x)\n",
    "    x = Flatten(name=\"value_flatten\")(x)\n",
    "    x = Dense(value_fc_size, kernel_regularizer=l2(l2_reg), activation=\"relu\", name=\"value_dense\")(x)\n",
    "    value_out = Dense(1, kernel_regularizer=l2(l2_reg), activation=\"tanh\", name=\"value_out\")(x)\n",
    "    \"\"\"\n",
    "    return Model(in_x, policy_out, name=\"west_model\")\n",
    "\n",
    "\n",
    "def build_residual_block(x, index,cnn_filter_num,l2_reg,cnn_filter_size):\n",
    "    in_x = x\n",
    "    res_name = \"res\"+str(index)\n",
    "    x = Conv2D(filters=cnn_filter_num, kernel_size=cnn_filter_size, padding=\"same\",\n",
    "              data_format=\"channels_first\", use_bias = False, kernel_regularizer=l2(l2_reg),\n",
    "              name=res_name+\"_conv1-\"+str(cnn_filter_size)+\"-\"+str(cnn_filter_num))(x)\n",
    "    x = BatchNormalization(axis=-1, name=res_name+\"_batchnorm1\")(x)\n",
    "    x = Activation(\"relu\", name=res_name+\"_relu1\")(x)\n",
    "    x = Conv2D(filters=cnn_filter_num, kernel_size=cnn_filter_size, padding=\"same\",\n",
    "              data_format=\"channels_first\", use_bias = False, kernel_regularizer=l2(l2_reg),\n",
    "              name=res_name+\"_conv2-\"+str(cnn_filter_size)+\"-\"+str(cnn_filter_num))(x)\n",
    "    x = BatchNormalization(axis=-1, name=res_name+\"_batchnorm2\")(x)\n",
    "    x = Add(name=res_name+\"_add\")([in_x,x])\n",
    "    x = Activation(\"relu\", name = res_name+\"_relu2\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 15:40:07.339300  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0124 15:40:07.357250  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0124 15:40:07.362238  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0124 15:40:07.385177  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0124 15:40:07.386173  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0124 15:40:09.628496  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0124 15:40:11.129514  6304 deprecation_wrapper.py:119] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(agents = [DQNAgent(0),DQNAgent(1),DQNAgent(2),DQNAgent(3)],trainer = Player(\"random\"),numGames = 100):\n",
    "    print(\"Training on\",numGames,\" games.\")\n",
    "    score_plot = []\n",
    "    counter_plot = []\n",
    "    old_state = [np.zeros([5,4,52]),np.zeros([5,4,52]),np.zeros([5,4,52]),np.zeros([5,4,52])]\n",
    "    for gm in range(numGames):\n",
    "        print(\"Playing game #\",gm+1,\" ...\")\n",
    "        g=Game(turn = random.randint(1,4))\n",
    "        for i in range(4):\n",
    "            agents[i].epsilon = numGames/2 - gm\n",
    "        while(not g.finish):\n",
    "            r = -1\n",
    "            turn = g.turn\n",
    "            ### save the vals of the table before the agent's turn\n",
    "            if(turn == 1):\n",
    "                old_state[g.turn-1] = agents[g.turn-1].get_state(g)\n",
    "                old_delta = g.score1 - g.score2\n",
    "            while(r==-1):\n",
    "                if random.randint(0, int(numGames/2)) < agents[g.turn-1].epsilon or turn != 1:\n",
    "                    m = random.randint(0,len(g.all_cards[g.turn-1])-1)\n",
    "                    final_move = keras.utils.to_categorical(m, num_classes=13)\n",
    "                    r = g.play_card(m)\n",
    "                else:\n",
    "                    old_state[g.turn-1].reshape(260)\n",
    "                    m = np.argmax(agents[g.turn-1].model.predict(old_state[g.turn-1].reshape(1,5,4,13)))\n",
    "                    final_move = keras.utils.to_categorical(m, num_classes=13)\n",
    "                    #print(\"hereee!!!\")\n",
    "                    r = g.play_card(m)\n",
    "                    while(r==-1):\n",
    "                        m = random.randint(0,len(g.all_cards[g.turn-1])-1)\n",
    "                        final_move = keras.utils.to_categorical(m, num_classes=13)\n",
    "                        r = g.play_card(m)\n",
    "            if(g.pack==1):\n",
    "                ### save what happened and the generated reward\n",
    "                ## each agent will get a part of memeory corresponding to what happend from their prespective\n",
    "                new_state = agents[g.turn-1].get_state(g)\n",
    "                new_delta = g.score1 - g.score2\n",
    "                reward = agents[0].set_reward(old_delta,new_delta)\n",
    "                agents[0].train_short_memory(old_state[0], final_move, reward, new_state, g.finish)\n",
    "                agents[0].remember(old_state[0], final_move, reward, new_state,g.finish)  \n",
    "                #for i in range(4):\n",
    "                 #   reward = agents[i].set_reward(old_delta,new_delta)\n",
    "                  #  agents[i].train_short_memory(old_state[i], final_move, reward, new_state, g.finish)\n",
    "                   # agents[i].remember(old_state[i], final_move, reward, new_state,g.finish)        \n",
    "        #for i in range(4):\n",
    "        agents[0].replay_new(agents[0].memory)\n",
    "        print(\"Game:\",gm+1,\"Scores:\",g.score1,g.score2,\"Aquired data:\",len(agents[0].memory))#sum(len(agents[0].memory) ))#for i in range(4)))\n",
    "        #score_plot.append(g.score2-g.score1)\n",
    "        #counter_plot.append(gm)\n",
    "        del(g)\n",
    "        #input()\n",
    "    print(\"Finished training!\")\n",
    "    #for i in range(4):\n",
    "    #    save_frozen_graph(agents[i].model,\"model\"+str(i))\n",
    "    #plot_seaborn(counter_plot, score_plot)\n",
    "    return agents[0].model,agents[1].model,agents[2].model,agents[3].model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1  games.\n",
      "Playing game # 1  ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 15:40:18.478291  6304 deprecation.py:323] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game: 1 Scores: 6 7 Aquired data: 13\n",
      "Finished training!\n"
     ]
    }
   ],
   "source": [
    "#p2 = Player(\"DQN\",agent=Agent,depth=20)\n",
    "#p1 = Player(\"random\",depth=5)\n",
    "#p2 = Player(\"random\",depth=5)\n",
    "#p3 = Player(\"random\",depth=5)\n",
    "#p4 = Player(\"random\",depth=5)\n",
    "#run(p1,p2,p3,p4,100)\n",
    "#Agent = DQNAgent()\n",
    "#trainer = Player(\"minmax\",depth=5)\n",
    "m1,m2,m3,m4 = train(numGames=1)\n",
    "#save_frozen_graph(m1,\"west1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model and Freezing graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5, 4, 13)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_conv5-32 (Conv2D)         (None, 32, 4, 13)    4000        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_batchnorm (BatchNormaliza (None, 32, 4, 13)    52          input_conv5-32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_relu (Activation)         (None, 32, 4, 13)    0           input_batchnorm[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res1_conv1-3-32 (Conv2D)        (None, 32, 4, 13)    9216        input_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res1_batchnorm1 (BatchNormaliza (None, 32, 4, 13)    52          res1_conv1-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res1_relu1 (Activation)         (None, 32, 4, 13)    0           res1_batchnorm1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res1_conv2-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res1_relu1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res1_batchnorm2 (BatchNormaliza (None, 32, 4, 13)    52          res1_conv2-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res1_add (Add)                  (None, 32, 4, 13)    0           input_relu[0][0]                 \n",
      "                                                                 res1_batchnorm2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res1_relu2 (Activation)         (None, 32, 4, 13)    0           res1_add[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res2_conv1-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res1_relu2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2_batchnorm1 (BatchNormaliza (None, 32, 4, 13)    52          res2_conv1-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2_relu1 (Activation)         (None, 32, 4, 13)    0           res2_batchnorm1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2_conv2-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res2_relu1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res2_batchnorm2 (BatchNormaliza (None, 32, 4, 13)    52          res2_conv2-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2_add (Add)                  (None, 32, 4, 13)    0           res1_relu2[0][0]                 \n",
      "                                                                 res2_batchnorm2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res2_relu2 (Activation)         (None, 32, 4, 13)    0           res2_add[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res3_conv1-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res2_relu2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3_batchnorm1 (BatchNormaliza (None, 32, 4, 13)    52          res3_conv1-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res3_relu1 (Activation)         (None, 32, 4, 13)    0           res3_batchnorm1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res3_conv2-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res3_relu1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res3_batchnorm2 (BatchNormaliza (None, 32, 4, 13)    52          res3_conv2-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res3_add (Add)                  (None, 32, 4, 13)    0           res2_relu2[0][0]                 \n",
      "                                                                 res3_batchnorm2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res3_relu2 (Activation)         (None, 32, 4, 13)    0           res3_add[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res4_conv1-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res3_relu2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4_batchnorm1 (BatchNormaliza (None, 32, 4, 13)    52          res4_conv1-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res4_relu1 (Activation)         (None, 32, 4, 13)    0           res4_batchnorm1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res4_conv2-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res4_relu1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res4_batchnorm2 (BatchNormaliza (None, 32, 4, 13)    52          res4_conv2-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res4_add (Add)                  (None, 32, 4, 13)    0           res3_relu2[0][0]                 \n",
      "                                                                 res4_batchnorm2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res4_relu2 (Activation)         (None, 32, 4, 13)    0           res4_add[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "res5_conv1-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res4_relu2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5_batchnorm1 (BatchNormaliza (None, 32, 4, 13)    52          res5_conv1-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res5_relu1 (Activation)         (None, 32, 4, 13)    0           res5_batchnorm1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res5_conv2-3-32 (Conv2D)        (None, 32, 4, 13)    9216        res5_relu1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "res5_batchnorm2 (BatchNormaliza (None, 32, 4, 13)    52          res5_conv2-3-32[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res5_add (Add)                  (None, 32, 4, 13)    0           res4_relu2[0][0]                 \n",
      "                                                                 res5_batchnorm2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "res5_relu2 (Activation)         (None, 32, 4, 13)    0           res5_add[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "policy_conv-1-2 (Conv2D)        (None, 2, 4, 13)     64          res5_relu2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "policy_batchnorm (BatchNormaliz (None, 2, 4, 13)     52          policy_conv-1-2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "policy_relu (Activation)        (None, 2, 4, 13)     0           policy_batchnorm[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "policy_flatten (Flatten)        (None, 104)          0           policy_relu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "policy_out (Dense)              (None, 13)           1365        policy_flatten[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 98,213\n",
      "Trainable params: 97,901\n",
      "Non-trainable params: 312\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0124 15:40:31.953593  6304 deprecation.py:323] From <ipython-input-9-9dfcf6fdd690>:31: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0124 15:40:31.954613  6304 deprecation.py:323] From c:\\users\\yazeed\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'model\\\\tf_model.pb'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = m1\n",
    "import os\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "model.save('./model/keras_model.h5')\n",
    "from keras import backend as K\n",
    "# This line must be executed before loading Keras model.\n",
    "K.set_learning_phase(0)\n",
    "from keras.models import load_model\n",
    "model = load_model('./model/keras_model.h5')\n",
    "model.summary()\n",
    "\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "sess = K.get_session()\n",
    "graph_def = sess.graph.as_graph_def()\n",
    "# graph_def\n",
    "#show_graph(graph_def)\n",
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    from tensorflow.python.framework.graph_util import convert_variables_to_constants\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        # Graph -> GraphDef ProtoBuf\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = convert_variables_to_constants(session, input_graph_def,\n",
    "                                                      output_names, freeze_var_names)\n",
    "        return frozen_graph\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in model.outputs])\n",
    "tf.train.write_graph(frozen_graph, \"model\", \"tf_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
